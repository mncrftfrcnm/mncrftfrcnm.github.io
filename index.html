<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Flan-T5 Inference Demo (Remote Model)</title>
  <!-- ONNX Runtime Web -->

  <!-- Transformers.js (Xenova) -->
<script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@3.3.0/dist/transformers.min.js"></script>

  <style>
    body { font-family: Arial, sans-serif; max-width: 600px; margin: 2rem auto; padding: 1rem; }
    h1 { text-align: center; }
    textarea { width: 100%; height: 80px; margin-bottom: 1rem; padding: 0.5rem; font-size: 1rem; }
    button { display: block; width: 100%; padding: 0.75rem; font-size: 1rem; cursor: pointer; border: none; border-radius: 0.25rem; background: #007bff; color: white; }
    pre { background: #f5f5f5; padding: 1rem; border-radius: 0.25rem; white-space: pre-wrap; }
  </style>
</head>
<body>
  <h1>Flan-T5 Inference Demo (Remote Model)</h1>
  <p>Model weights are served from a remote server; all computation happens in your browser, and weights are kept in memory (no persistent caching).</p>

  <textarea id="input" placeholder="what is the capital of great britain?"></textarea>
  <button id="generate">Generate</button>

  <h2>Output</h2>
  <pre id="output">(waiting...)</pre>
  <script type="module">


    async function init() {
      // Disable persistent caching (IndexedDB) so model lives only in memory
      env.useCache(false);

      // URL of your model directory on the server (must be CORS-enabled)
      const MODEL_URL = "https://huggingface.co/ShinpacheShimura/t5-smaller/resolve/main/";

      // Load tokenizer & quantized model remotely
      const tokenizer = await AutoTokenizer.from_pretrained(MODEL_URL, { localFilesOnly: false });
      const model = await AutoModelForSeq2SeqLM.from_pretrained(MODEL_URL, { quantized: true });

      document.getElementById("generate").addEventListener("click", async () => {
        const userInput = document.getElementById("input").value.trim();
        if (!userInput) return;
        const prompt = `Instruction: answer this request\nInput: ${userInput}\nOutput:`;

        // Tokenize + Generate
        const result = await model.generate({
          inputs: prompt,
          parameters: { max_new_tokens: 64, num_beams: 4 },
          options: { use_cache: true }
        });

        // Decode
        const answer = await tokenizer.decode(result[0], { skip_special_tokens: true });
        document.getElementById("output").innerText = answer;
      });
    }
    init();
  </script>
</body>
</html>
